# ğŸ“ Next Word Prediction â€” LSTM & GRU

A compact, practical repo demonstrating **next-word prediction** using two RNN variants:

- **`appLSTM.py`** â€” Next-word inference using a pretrained **LSTM** model  
- **`appGRU.py`** â€” Next-word inference using a pretrained **GRU** model

This README is based on the repository contents you provided and is a short, professional guide to understand and run the project.

---

## âœ… Highlights
- Two lightweight Streamlit apps for interactive next-word prediction.  
- Pretrained models included (`.h5`) plus tokenizer (`token.pickle`).  
- Simple pipeline: tokenization â†’ windowing/padding â†’ model inference.  
- Notebooks included for experiment & retraining.

---

## ğŸ“¦ Project Tree (core)
```

Next-Word-Prediction-using-GRU-LSTM/
â”œâ”€â”€ appLSTM.py               # Streamlit app: LSTM inference (main)
â”œâ”€â”€ appGRU.py                # Streamlit app: GRU inference (main)
â”œâ”€â”€ next_word_LSTM.h5        # Pretrained LSTM model
â”œâ”€â”€ next_word_GRU.h5         # Pretrained GRU model
â”œâ”€â”€ token.pickle             # Tokenizer (vocab â†’ index) required at inference
â”œâ”€â”€ hamlet.txt               # Sample corpus used for experiments/training
â”œâ”€â”€ experiements.ipynb       # LSTM training / experiments notebook
â”œâ”€â”€ experiements GRU.ipynb   # GRU training / experiments notebook
â””â”€â”€ requirements.txt         # Python dependencies

````

---

## ğŸ” Whatâ€™s inside (brief)
- **Models:** `next_word_LSTM.h5`, `next_word_GRU.h5` â€” Keras models saved after training.  
- **Tokenizer:** `token.pickle` â€” must match the model used.  
- **Corpus / Notebooks:** training & experimentation are documented in the included notebooks.  
- **Apps:** Streamlit-based UI for quick manual testing and demos.

---

## âš™ï¸ Requirements & environment
Install dependencies from the repo:

```bash
python -m venv .venv
# Activate:
# Windows: .venv\Scripts\activate
# macOS / Linux: source .venv/bin/activate

pip install -r requirements.txt
# If resource constrained: consider `pip install tensorflow-cpu`
````

---

## â–¶ï¸ Quick start â€” run locally

Run the **LSTM** app:

```bash
streamlit run appLSTM.py
```

Run the **GRU** app:

```bash
streamlit run appGRU.py
```

Open the provided Streamlit URL, type a short phrase, click **Predict Next Word** â€” the app returns the most probable next word.

---

## âš¡ How the apps work (one-liner)

Load `token.pickle` â†’ convert input phrase to token indices â†’ pad/trim to model input length â†’ `model.predict()` â†’ map predicted token index back to word.

---

## ğŸ”‘ Practical notes

* **Tokenizer & model must match**: always use the `token.pickle` that was used to train the respective model.
* Give **context (a few words)** for better next-word predictions.
* To **retrain or improve** results, open the notebooks, adjust corpus/parameters, and re-save `token.pickle` + model `.h5`.

---

## âš™ï¸ Retraining (brief)

Use the provided notebooks (`experiements.ipynb`, `experiements GRU.ipynb`) â€” they show preprocessing, model creation, training and saving of `token.pickle` and `.h5` model files.

---

## ğŸ“œ License

For **educational and research** purposes. Validate and test thoroughly before using in production.

---
